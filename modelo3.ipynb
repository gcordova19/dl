{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXADtnK5Dh9d",
        "outputId": "bcd943dd-dd12-4cdb-f0a0-ab2d6801ffe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "niKHWE93D5Xg"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy\n",
        "import pandas\n",
        "#import imageio.v3 as io\n",
        "\n",
        "#from tqdm import tqdm\n",
        "from typing import Optional, Union\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xc_JMyd-D-y_"
      },
      "outputs": [],
      "source": [
        "# Comprobamos a abrirlos de nuevo /content/drive/MyDrive/datos/filtered_data_Test.csv\n",
        "saved_data_Train = pandas.read_csv(\"/content/drive/MyDrive/datos/nuevo/filtered_data_Train.csv\", sep=';')\n",
        "saved_imgs_Train = numpy.load(\"/content/drive/MyDrive/datos/nuevo/imageTrain.npy\")\n",
        "saved_data_Train.shape, saved_imgs_Train.shape\n",
        "\n",
        "y_train = saved_data_Train.iloc[:, 0:1]   # Seleccionamos la primera columna, price\n",
        "X_train = saved_data_Train.iloc[:, 1:-1]  # Seleccionamos todas las columnas excepto la primera y la última\n",
        "\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "XtrainScaled = scaler.transform(X_train)\n",
        "\n",
        "X_train = XtrainScaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1mVDh1B-EDuX"
      },
      "outputs": [],
      "source": [
        "# Comprobamos a abrirlos de nuevo\n",
        "saved_data_Test = pandas.read_csv(\"/content/drive/MyDrive/datos/nuevo/filtered_data_Test.csv\", sep=';')\n",
        "saved_imgs_Test = numpy.load(\"/content/drive/MyDrive/datos/nuevo/imagesTest.npy\")\n",
        "saved_data_Test.shape, saved_imgs_Test.shape\n",
        "\n",
        "y_test = saved_data_Test.iloc[:, 0:1]      # nos quedamos con la 1ª columna, price\n",
        "X_test = saved_data_Test.iloc[:,1:-1]      # nos quedamos con el resto\n",
        "\n",
        "\n",
        "# recordad que esta normalización/escalado la realizo con el scaler anterior, basado en los datos de training!\n",
        "XtestScaled = scaler.transform(X_test)\n",
        "X_test = XtestScaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mW_nFSbJEMiZ"
      },
      "outputs": [],
      "source": [
        "# Comprobamos a abrirlos de nuevo\n",
        "saved_data_Val = pandas.read_csv(\"/content/drive/MyDrive/datos/nuevo/filtered_data_Val.csv\", sep=';')\n",
        "saved_imgs_Val = numpy.load(\"/content/drive/MyDrive/datos/nuevo/imagesVal.npy\")\n",
        "saved_data_Val.shape, saved_imgs_Val.shape\n",
        "\n",
        "y_val = saved_data_Val.iloc[:,0:1]     # nos quedamos con la 1ª columna, price\n",
        "X_val = saved_data_Val.iloc[:,1:-1]      # nos quedamos con el resto\n",
        "\n",
        "\n",
        "# recordad que esta normalización/escalado la realizo con el scaler anterior, basado en los datos de training!\n",
        "XvalScaled = scaler.transform(X_val)\n",
        "\n",
        "X_val = XvalScaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "83nErIKFEWMa"
      },
      "outputs": [],
      "source": [
        "# Cargar los datos\n",
        "X_train_img = saved_imgs_Train\n",
        "X_test_img = saved_imgs_Test\n",
        "X_val_img = saved_imgs_Val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8izHoGwKCpR"
      },
      "source": [
        "# Late-fusion\n",
        "La idea ha sigo pasar los datos a travez de arqutecturas sencillas con el fin de unir la salida y entonces aplicar una capa densa que devuelva la predicion del precio. En este caso se usa redes sencillas con el fin de agilizar el training del conjuto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSqpBJSRGsrn",
        "outputId": "92f3ae9f-2103-4603-c7e5-d60b9503b120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "423/423 [==============================] - 177s 414ms/step - loss: 1991.0874 - val_loss: 1846.2032\n",
            "Epoch 2/10\n",
            "423/423 [==============================] - 227s 535ms/step - loss: 1439.2205 - val_loss: 1888.2312\n",
            "Epoch 3/10\n",
            "423/423 [==============================] - 195s 460ms/step - loss: 1356.5546 - val_loss: 1663.4097\n",
            "Epoch 4/10\n",
            "423/423 [==============================] - 183s 433ms/step - loss: 1295.4856 - val_loss: 1637.1975\n",
            "Epoch 5/10\n",
            "423/423 [==============================] - 187s 441ms/step - loss: 1255.9296 - val_loss: 1612.7627\n",
            "Epoch 6/10\n",
            "423/423 [==============================] - 182s 431ms/step - loss: 1222.2620 - val_loss: 1571.6996\n",
            "Epoch 7/10\n",
            "423/423 [==============================] - 169s 400ms/step - loss: 1191.5258 - val_loss: 1596.1163\n",
            "Epoch 8/10\n",
            "423/423 [==============================] - 178s 422ms/step - loss: 1187.5475 - val_loss: 1539.9985\n",
            "Epoch 9/10\n",
            "423/423 [==============================] - 180s 425ms/step - loss: 1165.6250 - val_loss: 1529.3176\n",
            "Epoch 10/10\n",
            "423/423 [==============================] - 205s 486ms/step - loss: 1152.5723 - val_loss: 1556.1400\n",
            "27/27 [==============================] - 1s 51ms/step - loss: 1300.7448\n",
            "Mean Squared Error: 1300.7447509765625\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Definir la arquitectura de la red neuronal para las imágenes\n",
        "img_input = Input(shape=(224, 224, 3))\n",
        "x = Flatten()(img_input)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "img_output = Dense(64, activation='relu')(x)\n",
        "\n",
        "# Definir la arquitectura de la red neuronal para los datos tabulares\n",
        "tab_input = Input(shape=(X_train.shape[1],))\n",
        "x = Dense(64, activation='relu')(tab_input)\n",
        "tab_output = Dense(32, activation='relu')(x)\n",
        "\n",
        "# Combinar las salidas de ambas redes( se fusionan las salidas de las \n",
        "# redes neuronales convolucionales y las redes neuronales densas después de entrenarlas por separado)\n",
        "combined = concatenate([img_output, tab_output])\n",
        "x = Dense(32, activation='relu')(combined)\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "# Crear el modelo final\n",
        "model = Model(inputs=[img_input, tab_input], outputs=output)\n",
        "\n",
        "# Compilar el modelo\n",
        "learning_rate = 0.001\n",
        "opt = Adam(learning_rate=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=opt)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    [X_train_img, X_train], y_train,\n",
        "    validation_data=([X_val_img, X_val], y_val),\n",
        "    epochs=10, batch_size=16,\n",
        ")\n",
        "\n",
        "# Evaluar el modelo en los datos de prueba\n",
        "mse = model.evaluate([X_test_img, X_test], y_test)\n",
        "print(\"Mean Squared Error:\", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver en el transcurso de las epocas que se esta aprendiendo, ya que en train y validacion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT__4iC5KWjS"
      },
      "source": [
        "# Early-fusion\n",
        "Lo que haremos es la union de los datos, asi que los datos tabulares y las imagenes se aplanaran en un solo vextor de entrada y acontinuacion pasaran por dos capas densas con el fin de agilizar el training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Nr69opzwLzOa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import concatenate, Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT1K396mGu70"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Definir la arquitectura de la red de imágenes preentrenada\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "image_features = base_model(image_input)\n",
        "image_features = Flatten()(image_features)\n",
        "\n",
        "# Definir la arquitectura de la red para los datos tabulares\n",
        "tabular_input = Input(shape=(X_train.shape[1],))\n",
        "tabular_features = Dense(64, activation='relu')(tabular_input)\n",
        "\n",
        "# Fusionar los datos de entrada de imágenes y tabulares (En este punto tenemos un vector \n",
        "# procedente de la estracion de caracteristivas de VGG que se use al vector de datos tabulares )\n",
        "combined_input = concatenate([image_features, tabular_features])\n",
        "\n",
        "# Definir la arquitectura de la red neuronal. Aqui agregamos las capas densas para obtener la salida\n",
        "x = Dense(128, activation='relu')(combined_input)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "# Crear el modelo final\n",
        "model = Model(inputs=[image_input, tabular_input], outputs=output)\n",
        "\n",
        "# Compilar el modelo\n",
        "learning_rate = 0.001\n",
        "opt = Adam(learning_rate=learning_rate)\n",
        "model.compile(loss='mean_squared_error', optimizer=opt)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(\n",
        "    [X_train_img, X_train], y_train,\n",
        "    validation_data=([X_val_img, X_val], y_val),\n",
        "    epochs=10, batch_size=16,\n",
        ")\n",
        "\n",
        "# Evaluar el modelo en los datos de prueba\n",
        "mse = model.evaluate([X_test_img, X_test], y_test)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZJliHx5SyUt"
      },
      "source": [
        "la idea en este cuaderno es mostrar el conocimiento de los terminos aunque sea en una arquitectura simple. No se busca obtener los mejores resultados sino una configuracion acorde a los conceptos pedidos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
